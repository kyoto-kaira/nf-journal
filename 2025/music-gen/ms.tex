\section{はじめに}
\label{sec:introduction}
これまでの楽曲検索システムは曲名やアーティスト名、あるいはジャンルやテンポといったタグによる検索が主流でした。  
しかし、特定のタグやメタ情報だけでは「タグに存在しない感性的な要求」や楽曲の特徴からは直接推察できない要望に応えることが困難です。  
また、YouTube 等のプラットフォームでは話題性やランキングに基づいて検索結果が偏るため、純粋な類似性評価とは異なる順位付けが行われる場合があります。  
そこで本プロジェクトでは、自然言語クエリと音楽の類似度を直接測定するシステムを開発しました。
\section{アプリの機能}
\label{sec:features}
本システムは以下の主要機能を備えています。
\begin{itemize}
    \item \textbf{自然言語クエリ入力}：ユーザーは「激しい感情を歌った曲」や「リラックスできるメロディ」など、一般的な日本語表現で楽曲の特徴を指定できます。
    \item \textbf{CLAPモデルによる類似度測定}：CLAPモデルを用いて、テキストと音楽（音響特徴）間の類似度を計算します。
    \item \textbf{楽曲データベースとの照合}：事前に構築した楽曲データベース（J-POP中心）とクエリの埋め込みを比較し、類似度の高い楽曲を特定します。
    \item \textbf{類似度の可視化}：類似度に基づき楽曲をランキングし、2次元プロット等で可視化してユーザーに提示します。
\end{itemize}
これにより、ユーザーは従来のタグベース検索では見つけにくい、自身の意図に近い楽曲を効率的に探索できます。\\
\begin{figure}[h]
\begin{center}
\includegraphics[width=40mm]{music-gen/input.png}
\caption{クエリの入力例}
\end{center}
\end{figure}
\begin{figure}[h]
\begin{center}
\includegraphics[width=40mm]{music-gen/big_picture.png}
\caption{類似度の可視化結果}
\end{center}
\end{figure}
\section{推論・可視化の流れ}
\label{sec:inference_flow}
\subsection{概要}
ユーザーが自然言語で楽曲の特徴をクエリとして入力した際に、CLAPモデルを用いてテキストと音楽（音響特徴）の類似度を測定します。  
本節ではまず CLAP の基本構造と本研究での適用方針を説明し、その後で具体的な推論プロセスを示します。\\

\subsection{推論プロセス}
具体的な推論プロセスは以下の通りです。
\begin{enumerate}
    \item \textbf{クエリ入力と前処理}：ユーザーが「激しい感情を歌った曲」のような自然言語(日本語)クエリを入力します。
    \item \textbf{日本語から英語への翻訳}：Google Translation API を用いて英語に翻訳します。
    \item \textbf{テキスト・音楽埋め込みの生成}：翻訳したテキストとその他の多様なクエリ$X_s$を Text Encoder に、音源は前処理（メルスペクトログラム等）を行った上で Audio Encoder に入力し、埋め込みベクトル $E_{\text{query}}$ を生成します。
    \item \textbf{類似度の計算}：データベースに格納した楽曲の埋め込み $E_{\text{music},i}$ とクエリ埋め込み $E_{\text{query}}$ のコサイン類似度 $\mathrm{Sim}(E_{\text{music},i},E_{\text{query}})$ を計算し、各楽曲において、類似度スコアの平均で割ります。
    \item \textbf{検索結果の出力}：翻訳したテキストに関する類似度を基に楽曲をランキングし、類似度を座標軸として可視化してユーザーに提示します。
\end{enumerate}
この推論により、ユーザーは自身の意図した自然言語のニュアンスに近い楽曲群を探しやすくなります。\\
\subsection{CLAPモデルについて}
\label{subsec:clap}
CLAP\cite{elizalde2023clap}は、自然言語と音響を同一の埋め込み空間にマッピングすることを目的とした対照学習型モデルです。典型的には以下の構成を持ちます。
\begin{itemize}
  \item テキストエンコーダ（Text Encoder）とオーディオエンコーダ（Audio Encoder）を備えています。
  \item 対応するテキスト・音声ペアの埋め込みを近づけ、非対応ペアを遠ざける対照損失で学習します。
\end{itemize}
そのため、CLAP は「テキストで表現された音の概念」と「実際の音響特徴」を意味的に結びつけることができます。ゼロショット検索やテキスト条件付きの音検出に有効であり、本研究ではこれを日本語クエリと楽曲の類似度測定に応用します。\\
既存の CLAP は環境音や一般音声を中心に学習されていることが多く、楽曲固有のメロディや楽器、ハーモニーといった特徴を扱うには、対象ドメイン（例：J-POP）でのファインチューニングが有効です。そこで本研究では CLAP の Text/Audio Encoder を J-POP データで微調整し、日本語クエリに対して高精度な類似度評価が行えるように設計します。\\
\begin{figure}[h]
\begin{center}
\includegraphics[width=60mm]{music-gen/clap.png}
\caption{可視化結果}
\end{center}
\end{figure}
\section{学習の流れ}
\label{sec:training_flow}
\subsection{概要}
既存の CLAP モデル（例：laion/clap-htsat-fused）はクラシックや環境音中心に学習されているため、日本の楽曲に対する類似度測定には最適化されていません。  
本研究では J-POP を中心としたデータセットで CLAP をファインチューニングし、日本語クエリと楽曲埋め込みの対応を強化します。\\

\begin{enumerate}
 \item \textbf{データセットの構築}：日本の楽曲を対象に、楽曲ファイル（オーディオ）と対応するテキストデータ（YouTube コメント、歌詞、タグ等）を収集します。
 \item \textbf{ノイズ除去}：収集したテキストデータにはノイズが含まれるため、フィルタリングや前処理で不要な情報を除去します。
 \item \textbf{対照学習}：正例ペア（対応する楽曲とテキスト）間の類似度を最大化するように対照損失で学習します。
\end{enumerate}
この手順により CLAP は日本語の表現と楽曲の音響特徴との対応関係を深め、実用的な類似度評価が可能になります。\\
\subsection{データセットの構築}
学習用に使うデータセットとして必要なのは、プレビュー音源（オーディオ）、歌詞、タグ、曲の感想を表すコメントやレビューの情報(テキスト)です。\\
プレビュー音源(30秒)については、アーティスト名のリストを手動で作成し、それらをITunes Search APIによって検索をすることで取得します。各アーティストにつき上位５曲を検索します。曲の人気度や新しさなどに基づいて検索される曲が選ばれています。これにより、同時に曲のトラック情報やメタ情報を取得することができます。\\
歌詞については、Genius APIを用いてアーティスト名、曲のタイトル名で検索し、取得できる部分は取得します。Genius APIの検索で辿り着かない一部楽曲はmusic.jpというWebサイトをWebスクレイピングすることにより、取得します。以上の方法により楽曲の情報が取得できた楽曲を対象に、学習を行います。\\
楽曲の特徴や感想を表すコメントやレビューの情報の収集については、YouTube Data APIを用います。該当する楽曲のMVをアーティスト名、曲のタイトル名で検索し、そのコメント欄から各曲100件ずつ収集します。コメントは新しい順、人気順の二つのソートの方法がありますが、良質なコメントを多く集めたいので、人気順を選択しました。Youtubeのコメント欄は学習に有用な、曲の特徴を表したコメントばかりではありません。学習に不向きなコメントは次に解説する手順で除去することで、CLAP側のファインチューニングの精度の向上を試みました。\\
\subsection{ノイズとなるコメントの除去}
日本語版BERT\cite{sawada2024release}\cite{rinna-japanese-roberta-base}を用いてコメントを以下のラベルに分類し、ラベル0に該当するコメントのみ曲の特徴を表していると判断して残すことにしました。今回用いたモデルはRoBERTaをベースにしたものです。RoBERTaはNSP(次の文章の予測タスク)を削除したり、マスキングパターンを固定しないなど、BERTの学習手法をロバストに最適化されています。このモデルは今回のような分類タスクに優れています。\\
\begin{enumerate}
  \item \textbf{ラベル0}:曲の感想や、曲にまつわる投稿者のエピソード(例:儚いラブソングだと感じました。）
  \item \textbf{ラベル1}:曲に関連するMVやアニメ、ドラマなど、関連コンテンツに関するコメント(例:昨日のドラマはとてもよかった)
  \item \textbf{ラベル2}:曲の再生数や人気度に関するコメント(例:祝1億回再生!)
  \item \textbf{ラベル3}:アーティストのビジュアルや行動、エピソードに関するコメント(例:ドラマの主題歌書かせたらこの人がピカイチだね）
  \item \textbf{ラベル4}:曲に関係ない、雑多なコメント(例:毎日〇〇日記)
  \item \textbf{ラベル5}:歌詞
\end{enumerate}
BERTのファインチューニングには事前に教師データを用意しておく必要があります。教師データのラベリングについては手動で600件行いました。ラベル0や4が多くなり、ラベル５は少ないなど、ラベルの不均衡が起きていたため、少ないラベルに該当するデータをコピーして水増ししました。これによって、データが1200件まで増え、全てのラベルに属するデータを均等に学習することができます。\\
学習時では91％の正解率(accuracy)でラベル分類が行われました。全部のデータに手動で正解ラベルを与えたわけではないため、実際のコメントのラベル分類の正解率は正確には分かりませんが、これと同等の精度で分類できていると期待できます。実際、歌詞コメントなど、曲の内容に無関係なコメントは除去できていました。\\

\subsection{対照学習の方法}
\label{sec:contrastive_method}
本システムでは、\textbf{正例ペアのみを収集したデータセット}を用い、\textbf{InfoNCE Loss}に基づく対照学習を実行することで、モデルにオーディオとテキストの対応関係を学習させます。

\begin{enumerate}
    \item \textbf{日本語テキストの英訳と埋め込み}:
    CLAPモデルの事前学習を考慮し、収集した\textbf{日本語テキスト}は、学習前に\textbf{Google Translation API}を用いて英語に機械翻訳します。これにより、翻訳された英語テキスト $T$ と対応する楽曲データ $A$ のペア $(A, T)$ が学習の正例として用いられます。これらはそれぞれAudio EncoderとText Encoderに入力され、埋め込みベクトル $\boldsymbol{v}_A$ と $\boldsymbol{v}_T$ に変換されます。

    \item \textbf{損失関数 InfoNCE Loss の基盤}:
    損失関数には、バッチ内ネガティブサンプリング（In-batch Negative Sampling）の特性を持つ\textbf{InfoNCE Loss}（Information Noise-Contrastive Estimation Loss）を用います。この手法は、データセットから負例を明示的に用意しなくても、学習に利用されたミニバッチ内の正例でない組み合わせを自動的に負例として扱う仕組みです。

    具体的には、ミニバッチ内に $N$ 個の正例ペア $\{ (A_1, T_1), \dots, (A_N, T_N) \}$ があるとき、オーディオ $A_i$ に対するテキスト $T_i$ は\textbf{正例}ですが、それ以外のテキスト $T_j$ ($j \neq i$) は全て負例として機能します。これにより、$N$ 個の正例と $N(N-1)$ 個の負例が損失計算の過程で暗黙的に機能します。

    \item \textbf{対照損失の計算とモデルの学習}:
    InfoNCE Loss $\mathcal{L}_{A \to T}$ は、オーディオ埋め込み $\boldsymbol{v}_{A, i}$ が、バッチ内の $N$ 個のテキスト埋め込みの中から、正解テキスト $\boldsymbol{v}_{T, i}$ を正しく識別できる確率を最大化するように定義されます。コサイン類似度 $\text{sim}(\boldsymbol{v}_{A, i}, \boldsymbol{v}_{T, j})$ を用いた、オーディオ $A_i$ に基づく損失 $\mathcal{L}_{A \to T, i}$ は以下の通りです。
    \begin{align}
        \mathcal{L}_{A \to T, i} &= - \log \frac{\exp(\text{sim}(\boldsymbol{v}_{A, i}, \boldsymbol{v}_{T, i}) )}{\sum_{j=1}^{N} \exp(\text{sim}(\boldsymbol{v}_{A, i}, \boldsymbol{v}_{T, j}) )}
    \end{align}

    最終的な対照損失 $\mathcal{L}$は、オーディオからテキストへの損失 $\mathcal{L}_{A \to T}$ と、その逆 $\mathcal{L}_{T \to A}$ の双方向の和として計算され、これを最小化することで学習を行います。

    $\mathcal{L} = 0.5×\frac{1}{N}\sum_{i=1}^{N} (\mathcal{L}_{A \to T, i} + \mathcal{L}_{T \to A, i})$

    学習の目的は、正例ペア間の類似度を最大化し、負例として機能するバッチ内の非対応ペア間の類似度を最小化することで、エンコーダが楽曲の音響特徴と英訳されたテキスト表現の間の対応関係を学習することです。
\end{enumerate}
この手法でCLAPモデルをファインチューニングすることで、日本語クエリと楽曲の類似度測定に特化した性能向上がされました。\\
実際に学習後のモデルを用いて、自然言語クエリと楽曲の類似度測定を行った結果、ユーザーの意図に近い楽曲が学習前の時より上位にランクインするなど、実用的な性能が確認された。\\
\begin{table}[h]
\caption{「力強く歌うことで、葛藤から克服しようとしている」というクエリに対する類似度評価の比較}
\begin{center}
\begin{tabular}[t]{|c|c|c|}
\hline
\diagbox{楽曲}{モデル} & $変更前$ & $変更後$ \\
\hline
 残響賛歌  & $-0.9784$  & $0.9528$ \\
\hline
 天体観測  & $1.1011$ & $0.9412$ \\
\hline
 群青 & $-0.9766$ & $0.9510$ \\ 
\hline
\end{tabular}
\end{center}
\end{table}
\section{おわりに}
\label{sec:conclusion-music}
本研究では、自然言語クエリと楽曲オーディオを同一埋め込み空間で比較するシステムを提案しました。  
ユーザーは感性的な日本語表現を用いるだけで、自身の意図に近い楽曲を効率的に探索・可視化できます。  
しかし、「米津玄師みたいな曲」のような固有名詞を伴うクエリに対しては、CLAP の事前学習データに依存するため、十分な類似度評価が難しいため今後の課題となります。
今後はこういった面も含めて、CLAP のさらなる最適化、多ジャンル対応を行うことでより広範なユーザー体験の向上を目指します。

