\section{はじめに}
「目は口ほどに物を言う」という言葉が示すように、人間同士のコミュニケーションにおいて表情はきわめて大きな役割を担っています。しかし現在のヒューマン–コンピュータ・インタラクションにおいては、そのモダリティの多様性にもかかわらず、依然としてキーボード入力や音声指示といった言語的・運動的な情報に強く依存したままです。

もしこの制約を超え、表情そのものを操作インタフェースとして利用できれば、より自然で直感的な応答が可能となります。そこで本プロジェクトでは、顔表情認識を用いて操作するゲームを開発し、表情入力のユーザビリティと可能性を検証することを目的とします。

\section{使用ライブラリ・論文解説}
本プロジェクトでは、顔認識の基盤として Google が提供するMediaPipe Face Landmarker\cite{mediapipe_face_landmarker_2024} を採用しています。MediaPipe Face Landmarker は、顔の目や口といった主要ランドマークをCPU 環境でもリアルタイムに推定できる軽量・高精度なライブラリであり、表情入力を扱う本プロジェクトに非常に適しています。

このライブラリを利用することで、ユーザーの細かな表情変化を高い精度・低遅延で取得でき、自然かつ滑らかな操作体験を実現できる点が大きな利点です。

MediaPipe Face Landmarker は、Google が開発した軽量顔検出器 BlazeFace\cite{bazarevsky2019blazeface} を基盤として構築されています。本節は、その論文をもとに、その主要なアーキテクチャ上の特徴を概説します。

BlazeFace は、モバイル GPU 上での推論に特化して設計された軽量かつ高速な顔検出モデルです。スマートフォン上で 200–1000 FPS という極めて高い速度で動作し、AR アプリケーションなどで求められる 2D/3D ランドマーク推定や表情分類へほぼ遅延なく顔領域を提供できる点が特長です。

この革新的な性能は、以下の三つの主要なアーキテクチャ上の工夫によって実現されています。

\subsection{軽量な特徴抽出ネットワーク}
MobileNetV1/V2\cite{howard2017mobilenets}\cite{sandler2018mobilenetv2}に着想を得ながらも、モバイルでの物体検出に最適化された独自のコンパクトなネットワークを採用しています。このネットワークでは、図\ref{fig:blazeface_architecture}に示すように、一般的な3x3カーネルの代わりに5x5カーネルを用いた深度方向分離可能畳み込み（depthwise separable convolution）をモデルのボトルネック部分で活用します。これにより、演算量を抑えつつも、より少ない層で広い受容野（receptive field）を確保し、効率的な特徴抽出を可能にしています。 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{face-game/img/blazeface_architecture.png}
    \caption{BBlazeBlock（左）とダブルBlazeBlock（右）のアーキテクチャ}
    \label{fig:blazeface_architecture}
\end{figure}

\subsection{GPUに最適化されたアンカー方式}
BlazeFaceでは、GPUの処理特性に合わせた独自のアンカー生成方式が採用されています。一般的なモデルが特徴マップを1×1まで段階的に縮小していくのに対し、BlazeFaceは図\ref{fig:blazeface_anchors}に示すように解像度を8×8で止め、それ以上のダウンサンプリングを行いません。

これは、低解像度の特徴マップを扱う際に発生する GPUディスパッチの固定オーバーヘッドを削減し、推論速度の大幅な向上につながる設計です。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{face-game/img/blazeface_anchors.png}
    \caption{BlazeFaceのアンカー方式}
    \label{fig:blazeface_anchors}
\end{figure}

\subsection{後処理でのtie resolution戦略}
物体検出では多くの場合、複数のバウンディングボックスの候補が同一の物体を指す場合にNMS（Non-Maximum Suppression）を用いて1つのバウンディングボックスに絞り込みますが、BlazeFaceはNMSに代えて 独自のブレンディング戦略を採用しています。

複数のバウンディングボックスの重み付き平均を取ることで最終出力を決定し、フレーム間での検出結果のちらつき（時間的ジッター）を大幅に低減します。その結果、より滑らかで安定した検出が実現されます。

\subsection{まとめ}
これらの工夫により BlazeFace は、モバイル環境における顔検出の速度と精度の基準を大きく引き上げました。そしてその成果は MediaPipe Face Landmarkerを通じて、現在の多くの AR・表情認識アプリケーションの基盤技術となっています。

\section{ゲーム入力の実装}
本節では、MediaPipe ライブラリを用いた表情認識によるゲーム入力の実装について、主に工夫した箇所に絞って解説していきます。

\subsection{入力の詳細}
今回はリアルタイム推論を行うので、比較的検出しやすい「まばたきをする」「口を開ける」「笑顔になる」の3つの表情入力を対象とします。各表情は立ち上がり時（しきい値を下回る状態から上回った瞬間）に一度だけ入力として扱います。長押しのような操作は存在しません。

\subsection{軽量化・遅延削減}
今回の実装において、肝となるのはやはり軽量化・遅延削減です。ゲーム入力にはリアルタイム性が求められるため、表情認識をいかに素早く処理してゲームに反映させるかが重要になってきます。最適化を行わなかった初期実装では、ゲーム画面がカクついて到底快適ではありませんでした。画面にカクつきが生じるのは、1フレーム当たりの推論時間が描画間隔に対して間に合っていなかったのが原因であると考えられます。そこで実装したのが非同期推論です。

非同期推論とは、ゲームの描画とは別の時間軸で表情認識の推論を行う機能です。初期実装では、1フレームの推論が完了してから次のフレームの描画を始めていました。対して非同期推論では、1フレームの推論の進捗に関わらず、次のフレーム描画を始めます。安定したフレームレートが重視されるゲーム描画には、この非同期推論は有効でしょう。表情認識処理とは無関係に描画が行われるので、画面のカクつきはこれで解消することができます

では実際、どのように非同期推論を実装しているのかについても少し触れておきます。

MediaPipe Face Landmarkerには、標準機能で非同期推論を行うLIVE STREAMモードが搭載されています。今回の場合では、メインの描画処理とは別スレッドで、このLIVE STREAMモードで表情認識処理を行い、その処理結果を入力として採用しています。

さて、非同期推論を導入したことで画面のカクつきは解消されました。しかし、このままではもうひとつ問題があります。表情認識処理が非同期になった分、入力の遅延が発生してしまったのです。以前までの同期推論では、表情認識処理が完全に終了してから次のフレームが描画されるため、遅延の問題は発生しませんでした。そこで、次は推論自体を軽量化することで、遅延の解消に取り組みます。

具体的には、解像度の縮小とフレームレートの削減を行っています。表情認識処理に入力する画像の解像度を下げることで、処理は軽量になりますが、そのぶん精度の低下にもつながります。しかし、今回は「口を開ける」「まばたきをする」「笑顔になる」という比較的検出しやすい3つの特徴を区別できればよく、解像度を80×60ピクセル（幅×高さ）まで下げてもプレイにはあまり問題は生じませんでした。同様にフレームレートに関しても、60fpsから15fpsまで下げることで軽量化と精度の維持を両立しています。

加えて、カメラのバッファサイズの削減、MJPEG の使用、CPU/GPU デリゲートの切り替えにも対応しました。これらにより、入力タイミングの精度が求められるrunnerゲームにおいても、遅延を気にせず快適にプレイできる水準を実現しました。

\subsection{検知精度の向上}
先ほどまでは軽量化・遅延削減の工夫を解説してきましたが、次に表情入力検知について紹介します。「まばたきをする」「口を開ける」「笑顔になる」の3入力それぞれに関して、検知条件などを解説していきます。

まず、「まばたき」の検出に関しては、右目と左目における「まばたきの程度（eyeBlink）」の平均値に加えて、補助的に「目の細め具合（eyeSquint）」の平均値も用いています。これらの平均値のうち最大値を使用することで比較的高精度で「まばたき」を検出することができました。

次に、「口を開ける」の検出には「あごの開き具合（jawOpen）」と「口の開き具合（1 - mouthClose）」の二つを用いています。「あごの開き具合（jawOpen）」は数値が出ないことがあったので、予備の条件として「口の開き具合（1 - mouthClose）」を使用しています。

最後に、「笑顔」の検出には「左右の口角の上がり具合（mouthSmileLeft/Right）」を用いたうえで、予備の条件として、「左右の口角の引き上げ量（mouthCornerPullLeft/Right）」も用いています。残念ながら口角を動かさずに笑う人の笑顔は検出できなさそうですね。

これらに加え、検出のON/OFF に異なるしきい値を設けるヒステリシスにより、誤検知やチャタリングを抑制しています。

\section{制作ゲームの紹介}
表情認識を用いて制作したゲームのうち、代表的な2作を紹介します。

\subsection{Runner}
当研究会のマスコットキャラクター「KaiRA 君」を操作し、迫り来る障害物を避ける横スクロールアクション。「口を開ける」とジャンプし、タイミングよく障害物を回避します。どこか某ブラウザの恐竜ゲームを想起させる内容です。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{face-game/img/runner_game.png}
    \caption{runnerゲームのプレイ画面}
    \label{fig:runner_game}
\end{figure}

\subsection{Speed React}
次々と提示される状況に対して、素早く正確なリアクションを求められる独創的なゲーム（バカゲー）。「笑顔」「驚いた顔」「真顔」のうち状況に合ったリアクションを行います。2人対戦が可能な「VS React」も用意しています。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{face-game/img/speed_react_game.png}
    \caption{Speed Reactゲームのプレイ画面}
    \label{fig:speed_react_game}
\end{figure}

\section{おわりに}
本プロジェクトでは、表情認識を用いたゲーム操作を通じて、人間とコンピュータの新しい関わり方の可能性を検討しました。実装を進めるなかで強く感じたのは、表情を入力として扱うことは想像以上に難しいという点です。その理由のひとつは、表情がしばしば意図的な操作ではなく、無意識の反応として現れる情報だからです。

キーボードやマウスのような明確な「命令」とは異なり、表情は感情や注意の変化、緊張や驚きといった非言語的で曖昧な状態をそのまま映し出します。したがって、表情をインタフェースとして用いることは、単なる操作系の設計を超えて、人間の身体性や無意識との向き合い方を考える営みでもあると感じました。

しかし同時に、もし将来「無意識のコンピューティング」とでも呼べるような新しいインタラクションの枠組みが成立するとすれば、そのもっとも素朴で原初的な入口は、表情によるインプットなのかもしれません。意図と言語を介さず、身体が自然に発する信号をそのまま拾い上げることができる点で、表情は人間の奥深いレイヤーに最も近い情報だからです。

今回の試みは小さな実装にすぎませんが、ヒューマン–コンピュータ・インタラクションがより「人間そのもの」に寄り添う方向へ進む可能性を垣間見ることができました。今後は、表情だけでなく姿勢や視線、呼吸などのマルチモーダルな信号を統合することで、より自然で直感的なインタラクションが実現できると考えています。本プロジェクトがその一歩となれば幸いです。
